{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2830d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os # NEW IMPORT for file system operations\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_fscore_support, auc, precision_recall_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight, resample # NEW IMPORT for downsampling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "SIGNAL_LENGTH = 187\n",
    "NUM_CLASSES = 5\n",
    "KFOLD_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "MAX_EPOCHS = 25 # OPTIMIZATION: Reduced from 50\n",
    "BATCH_SIZE = 128 # OPTIMIZATION: Increased from 64 for faster training\n",
    " \n",
    "RAW_DATA_PATH = '/content/mitbih_database/'\n",
    "\n",
    "RECORD_IDS = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 200, 201, 202, 203, 205, 207, 208, 209, 210, 212, 213, 214, 215, 217, 219, 220, 221, 222, 223, 228, 230, 231, 232, 233, 234]\n",
    "\n",
    "AAMI_MAPPING = {\n",
    "    'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # N (Normal)\n",
    "    'A': 1, 'a': 1, 'J': 1, 'S': 1,          # S (Supraventricular ectopic)\n",
    "    'V': 2, 'E': 2,                          # V (Ventricular ectopic)\n",
    "    'F': 3,                                  # F (Fusion)\n",
    "    '/': 4, 'f': 4, 'Q': 4, '?': 4           # Q (Unclassifiable/Paced)\n",
    "}\n",
    "\n",
    "# AAMI Class Mapping: N=0, S=1, V=2, F=3, Q=4\n",
    "CLASS_LABELS = {0: 'N (Normal)', 1: 'S (SVEB)', 2: 'V (VEB)', 3: 'F (Fusion)', 4: 'Q (Unknown)'}\n",
    "CLASS_COLORS = {0: 'green', 1: 'blue', 2: 'orange', 3: 'red', 4: 'purple'}\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "\n",
    "    if not os.path.isdir(RAW_DATA_PATH):\n",
    "        print(\"CRITICAL ERROR: The data directory was NOT found.\")\n",
    "        print(f\"Please check the path: --> {RAW_DATA_PATH}\")\n",
    "        return np.array([]), np.array([])\n",
    "    else:\n",
    "        file_list = os.listdir(RAW_DATA_PATH)\n",
    "        print(f\"Directory found. Total files: {len(file_list)}. Sample files: {file_list[:5]}...\")\n",
    "\n",
    "    all_beats = []\n",
    "    all_labels = []\n",
    "\n",
    "    \n",
    "    for rec_id in RECORD_IDS:\n",
    "       \n",
    "        signal_file = os.path.join(RAW_DATA_PATH, f'{rec_id}.csv')\n",
    "        \n",
    "        annotation_file = os.path.join(RAW_DATA_PATH, f'{rec_id}annotations.txt')\n",
    "        \n",
    "        if not os.path.exists(signal_file) or not os.path.exists(annotation_file):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "           \n",
    "          \n",
    "            signal_df = pd.read_csv(signal_file, header=0, engine='python')\n",
    "            \n",
    "            \n",
    "            if 'mlii' in signal_df.columns:\n",
    "                 signal = signal_df['mlii'].values\n",
    "            elif signal_df.shape[1] > 1:\n",
    "                # Fallback to the second column\n",
    "                signal = signal_df.iloc[:, 1].values\n",
    "            else:\n",
    "                signal = signal_df.iloc[:, 0].values\n",
    "\n",
    "           \n",
    "            annotations = pd.read_csv(\n",
    "                annotation_file, \n",
    "                sep=r'\\s+', \n",
    "                header=None,\n",
    "                skiprows=1, \n",
    "                usecols=[1, 2], \n",
    "                names=['Index', 'Beat_Type'],\n",
    "                on_bad_lines='skip'\n",
    "            )\n",
    "            r_peak_indices = annotations['Index'].values.astype(int)\n",
    "            beat_types = annotations['Beat_Type'].values\n",
    "\n",
    "        except Exception as e:\n",
    "            # Skip records with I/O errors\n",
    "            print(f\"Warning: Error processing record {rec_id}. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "       \n",
    "        half_beat = SIGNAL_LENGTH // 2 \n",
    "        \n",
    "        for r_idx, beat_type in zip(r_peak_indices, beat_types):\n",
    "            label = AAMI_MAPPING.get(beat_type.strip(), 4) \n",
    "            \n",
    "            start_idx = r_idx - half_beat\n",
    "            end_idx = r_idx + half_beat + 1 \n",
    "            \n",
    "            if start_idx < 0 or end_idx > len(signal) or len(signal[start_idx:end_idx]) != SIGNAL_LENGTH:\n",
    "                continue\n",
    "\n",
    "            beat_segment = signal[start_idx:end_idx]\n",
    "\n",
    "            all_beats.append(beat_segment)\n",
    "            all_labels.append(label)\n",
    "\n",
    "    X_raw = np.array(all_beats)\n",
    "    Y_raw = np.array(all_labels)\n",
    "    \n",
    "    if X_raw.shape[0] == 0:\n",
    "        print(\"CRITICAL ERROR: No beats were extracted...\")\n",
    "        return X_raw, Y_raw\n",
    "\n",
    "    X_final = []\n",
    "    Y_final = []\n",
    "    \n",
    "   \n",
    "    data_df = pd.DataFrame(X_raw)\n",
    "    data_df['label'] = Y_raw\n",
    "    \n",
    "\n",
    "    df_majority = data_df[data_df['label'] == 0]\n",
    "    df_minority = data_df[data_df['label'] != 0]\n",
    "    \n",
    "    N_MAX_SAMPLES = 20000 \n",
    "    \n",
    "   \n",
    "    df_majority_downsampled = resample(\n",
    "        df_majority,\n",
    "        replace=False, \n",
    "        n_samples=min(len(df_majority), N_MAX_SAMPLES), \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Combine downsampled majority class with all minority classes\n",
    "    df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "    \n",
    "    # Shuffle the final dataset\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    X = df_balanced.drop('label', axis=1).values\n",
    "    Y = df_balanced['label'].values\n",
    "    \n",
    "    \n",
    "    # 7. Apply Z-Score Normalization\n",
    "    X = np.apply_along_axis(stats.zscore, 1, X)\n",
    "\n",
    "    # 8. Reshape for 1D CNN: (samples, timesteps, features=1)\n",
    "    X = X.reshape(X.shape[0], SIGNAL_LENGTH, 1)\n",
    "\n",
    "    print(f\"--- Finished Segmentation and Downsampling ---\")\n",
    "    print(f\"Total Samples (Original): {X_raw.shape[0]}, Total Samples (Final): {X.shape[0]}\")\n",
    "    print(f\"Final Class Distribution: {Counter(Y)}\")\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "   \n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv1D(filters=32, kernel_size=5, padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Block 2\n",
    "        Conv1D(filters=64, kernel_size=5, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Block 3\n",
    "        Conv1D(filters=128, kernel_size=5, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Classifier\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    \"\"\"\n",
    "    Calculates class weights inversely proportional to class frequencies.\n",
    "    This is essential for handling the severe class imbalance in MIT-BIH.\n",
    "    \"\"\"\n",
    "    weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "   \n",
    "    class_weights_dict = dict(enumerate(weights))\n",
    "    print(\"\\nCalculated Class Weights (Higher for Rare Classes):\")\n",
    "    for k, v in class_weights_dict.items():\n",
    "        print(f\"  Class {k} ({CLASS_LABELS[k]}): {v:.4f}\")\n",
    "    return class_weights_dict\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred_probs, class_labels):\n",
    "\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred_classes, average='macro', zero_division=0.0) \n",
    "    recall_per_class = recall_score(y_true, y_pred_classes, average=None, labels=range(len(class_labels)), zero_division=0.0)\n",
    "\n",
    "    pr_auc_scores = []\n",
    "    for i in range(len(class_labels)):\n",
    "       \n",
    "        if np.sum(y_true == i) == 0:\n",
    "            pr_auc = np.nan # Use NaN if class is absent in test set\n",
    "        else:\n",
    "            precision, recall, _ = precision_recall_curve(y_true == i, y_pred_probs[:, i])\n",
    "            pr_auc = auc(recall, precision)\n",
    "        pr_auc_scores.append(pr_auc)\n",
    "    \n",
    "   \n",
    "    mean_pr_auc = np.nanmean(pr_auc_scores) \n",
    "\n",
    "    results = {\n",
    "        'F1_Macro': f1_macro,\n",
    "        'Recall_Per_Class': recall_per_class,\n",
    "        'PR_AUC_Mean': mean_pr_auc,\n",
    "        'PR_AUC_Per_Class': pr_auc_scores,\n",
    "        'Confusion_Matrix': confusion_matrix(y_true, y_pred_classes)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \n",
    "    X, Y = load_and_preprocess_data()\n",
    "    \n",
    "    \n",
    "    if X.shape[0] == 0:\n",
    "        return\n",
    "\n",
    "    y_encoded = to_categorical(Y, num_classes=NUM_CLASSES)\n",
    "\n",
    "    class_weights = calculate_class_weights(Y)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    fold_metrics = []\n",
    "    \n",
    "    print(f\"\\n--- Starting {KFOLD_SPLITS}-Fold Stratified Cross-Validation ---\")\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
    "        print(f\"\\n[FOLD {fold + 1}/{KFOLD_SPLITS}]\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        Y_train_enc, Y_test_enc = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        # NEW PRINT STATEMENT: Show data size for the current fold\n",
    "        print(f\"  Data Split: Train samples={len(X_train)}, Test samples={len(X_test)}\")\n",
    "        print(f\"  Training distribution: {Counter(Y_train)}\")\n",
    "        \n",
    "        \n",
    "        model = create_cnn_model(X_train.shape[1:], NUM_CLASSES)\n",
    "        \n",
    "       \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        print(f\"  Training Model (Max Epochs={MAX_EPOCHS}, Batch Size={BATCH_SIZE})...\")\n",
    "        \n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, Y_train_enc,\n",
    "            epochs=MAX_EPOCHS, # OPTIMIZATION APPLIED HERE\n",
    "            batch_size=BATCH_SIZE, # OPTIMIZATION APPLIED HERE\n",
    "            validation_data=(X_test, Y_test_enc),\n",
    "            class_weight=class_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1 # CHANGED FROM 0 TO 1 for visibility\n",
    "        )\n",
    "        print(f\"  Trained for {len(history.history['loss'])} epochs. Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "       \n",
    "        Y_pred_probs = model.predict(X_test, verbose=0)\n",
    "        \n",
    "       \n",
    "        metrics = evaluate_metrics(Y_test, Y_pred_probs, CLASS_LABELS)\n",
    "        metrics['Fold'] = fold + 1\n",
    "        fold_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"  F1-Macro: {metrics['F1_Macro']:.4f}, Mean PR-AUC: {metrics['PR_AUC_Mean']:.4f}\")\n",
    "        # Focus on rare classes\n",
    "        print(f\"  Rare Class Recall: F ({metrics['Recall_Per_Class'][3]:.3f}), Q ({metrics['Recall_Per_Class'][4]:.3f})\")\n",
    "\n",
    "\n",
    "    # --- Final Report Generation ---\n",
    "    final_report = {\n",
    "        'F1_Macro': [],\n",
    "        'PR_AUC_Mean': [],\n",
    "        'Recall_N': [], 'Recall_S': [], 'Recall_V': [], 'Recall_F': [], 'Recall_Q': [],\n",
    "        'PR_AUC_N': [], 'PR_AUC_S': [], 'PR_AUC_V': [], 'PR_AUC_F': [], 'PR_AUC_Q': [],\n",
    "    }\n",
    "    \n",
    "    for fm in fold_metrics:\n",
    "        final_report['F1_Macro'].append(fm['F1_Macro'])\n",
    "        final_report['PR_AUC_Mean'].append(fm['PR_AUC_Mean'])\n",
    "        \n",
    "        # Per-class recall\n",
    "        final_report['Recall_N'].append(fm['Recall_Per_Class'][0])\n",
    "        final_report['Recall_S'].append(fm['Recall_Per_Class'][1])\n",
    "        final_report['Recall_V'].append(fm['Recall_Per_Class'][2])\n",
    "        final_report['Recall_F'].append(fm['Recall_Per_Class'][3])\n",
    "        final_report['Recall_Q'].append(fm['Recall_Per_Class'][4])\n",
    "\n",
    "        # Per-class PR-AUC\n",
    "        final_report['PR_AUC_N'].append(fm['PR_AUC_Per_Class'][0])\n",
    "        final_report['PR_AUC_S'].append(fm['PR_AUC_Per_Class'][1])\n",
    "        final_report['PR_AUC_V'].append(fm['PR_AUC_Per_Class'][2])\n",
    "        final_report['PR_AUC_F'].append(fm['PR_AUC_Per_Class'][3])\n",
    "        final_report['PR_AUC_Q'].append(fm['PR_AUC_Per_Class'][4])\n",
    "\n",
    "    \n",
    "    final_metrics_data = {}\n",
    "    for k, v in final_report.items():\n",
    "        # Sanitize: Convert list elements to numeric, coercing non-numeric/strings to NaN\n",
    "        sanitized_v = pd.to_numeric(np.asarray(v), errors='coerce') \n",
    "        final_metrics_data[k] = [np.nanmean(sanitized_v), np.nanstd(sanitized_v)]\n",
    "\n",
    "    results_df = pd.DataFrame(final_metrics_data, index=['Mean', 'Std'])\n",
    "    results_df = results_df.T\n",
    "    \n",
    "    print(\"\\n=======================================================================\")\n",
    "    print(\"                 FINAL 1D-CNN CLASSIFICATION REPORT\")\n",
    "    print(\"=======================================================================\")\n",
    "    print(f\"Cross-Validation: {KFOLD_SPLITS}-Fold Stratified CV\")\n",
    "    print(\"Imbalance Handled: Class Weighting + N-Class Downsampling\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    \n",
    "    # Display overall metrics\n",
    "    print(f\"Overall F1-Macro: {results_df.loc['F1_Macro', 'Mean']:.4f} (+/- {results_df.loc['F1_Macro', 'Std']:.4f})\")\n",
    "    print(f\"Mean PR-AUC:      {results_df.loc['PR_AUC_Mean', 'Mean']:.4f} (+/- {results_df.loc['PR_AUC_Mean', 'Std']:.4f})\")\n",
    "    \n",
    "    print(\"\\n--- Per-Class Recall (Crucial for Rare-Classes) ---\")\n",
    "    \n",
    "    for i, label in CLASS_LABELS.items():\n",
    "        key = f'Recall_{label.split(\" \")[0]}'\n",
    "        mean_recall = results_df.loc[key, 'Mean']\n",
    "        std_recall = results_df.loc[key, 'Std']\n",
    "        print(f\"  {label:<18} (Class {i}): {mean_recall:.4f} (+/- {std_recall:.4f})\")\n",
    "\n",
    "    print(\"\\n--- Per-Class PR-AUC ---\")\n",
    "\n",
    "    for i, label in CLASS_LABELS.items():\n",
    "        key = f'PR_AUC_{label.split(\" \")[0]}'\n",
    "        mean_pr_auc = results_df.loc[key, 'Mean']\n",
    "        std_pr_auc = results_df.loc[key, 'Std']\n",
    "        print(f\"  {label:<18} (Class {i}): {mean_pr_auc:.4f} (+/- {std_pr_auc:.4f})\")\n",
    "    \n",
    "    print(\"=======================================================================\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Set logging off for a cleaner output\n",
    "    tf.get_logger().setLevel('ERROR') \n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
